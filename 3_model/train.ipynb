{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5aacb-2f20-4d0a-9010-92e72d13b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sinfo -p gpu\n",
    "# !nvidia-smi\n",
    "# !free -h\n",
    "# !df -h ~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92282a-8f64-4d07-b6d5-dea5959b28f8",
   "metadata": {},
   "source": [
    "# VIPAA=Violence, Impunity, and Peace in the Americas Archive\n",
    "### domain-specific dataset compiled at the University of Arizona (School of Government & Public Policy, with NSF support) for computational research on political violence and human rights across Latin America.\n",
    "\n",
    "# https://www.colombiaarmedactors.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61477b-faa4-4a62-bb52-08b63ea7dec5",
   "metadata": {},
   "source": [
    "#short description\n",
    "Fine-tunning conflibert on the VIPAA corpus to perform multi-label classification of violent event descriptions in Spanish\n",
    "The source dataset (45,512 instances) was preprocessed by extracting human-readable labels from the column Tipificación_with_codes, removing numerical prefixes (e.g., “D:4:701”) and normalizing orthography. Sixty-five unique labels from list_of_labels.xlsx were used to build a binary multi-hot vector representation matrix for training. We used the continuation checkpoint snowood1/ConfliBERT-cont-uncased (Hugging Face, 2023) as the base model, fine-tuned with a binary cross-entropy loss and the Hugging Face Trainer API (Transformers v4.46.3, Accelerate v1.0.1, PyTorch v2.4.1) on the UA HPC system. Evaluation used micro- and macro-averaged F1, precision, and recall, with a tuned sigmoid threshold (≈ 0.4) to optimize micro-F1 performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b829107-578e-44f0-83e2-971cfe7b2c2a",
   "metadata": {},
   "source": [
    "# Përshkrim i procesit dhe skriptit\n",
    "##  **E përgjitshme**\n",
    "\n",
    "We prepared and fine-tuned a **multi-label ConfliBERT classifier** to automatically assign violence-event categories (e.g., *ASESINATO*, *BIENES CIVILES*, *PILLAJE*, etc.) to text descriptions of incidents from the **VIPAA corpus** (~45 K Spanish event reports downloaded from nocheyniebla.org).\n",
    "\n",
    "The workflow covered full preprocessing, label normalization, and fine-tuning of **snowood1/ConfliBERT-cont-uncased** under the UA HPC environment.\n",
    "\n",
    "##  **Çdo hap**\n",
    "\n",
    "### 1 Data preparation\n",
    "\n",
    "* Two Excel files were used:\n",
    "\n",
    "  * **vipaa_for_training.xlsx** — main dataset with event descriptions and coded labels.\n",
    "  * **list_of_labels.xlsx** — master list of 65 standardized action labels (actions_clean column).\n",
    "* Both were loaded into Pandas DataFrames.\n",
    "\n",
    "### 2 Label extraction and cleaning\n",
    "\n",
    "* The relevant column was **Tipificacion_with_codes** (entries like\n",
    "  \"D:2:80 BIENES CIVILES, D:2:801 ATAQUE A OBRAS E INST. QUE CONT. FUERZAS PELIGR.\").\n",
    "* We removed numeric codes (B:2:40, D:4:701, etc.) using a regular-expression parser, yielding clean upper-case labels such as:\n",
    "\n",
    "  text\n",
    "  ['BIENES CIVILES', 'ATAQUE A OBRAS E INST. QUE CONT. FUERZAS PELIGR.']\n",
    "  \n",
    "* Nested or repeated lists (e.g., [[\"A\",\"B\"]] or [A,A,A]) were flattened and deduplicated:\n",
    "\n",
    "  python\n",
    "  data_df[label_col] = data_df[label_col].apply(lambda lst: sorted(set(lst)))\n",
    "  \n",
    "\n",
    "### 3 Normalization and mapping\n",
    "\n",
    "* Accents were stripped (unicodedata.normalize), text upper-cased, and matched to the 65 canonical labels in list_of_labels.xlsx.\n",
    "* A **label->ID** dictionary (label_map) and **ID->label** reverse map (id2label) were created.\n",
    "* Each event's label list was converted to numeric IDs (e.g., [6, 36]).\n",
    "\n",
    "### 4 Multi-hot encoding\n",
    "\n",
    "* A binary matrix y ∈ ℝ^{45511×65} was built, where 1 marks presence of a label.\n",
    "  Example: row ₀ -> 1 for columns 6 and 36.\n",
    "\n",
    "### 5 Environment setup on UA HPC\n",
    "\n",
    "* Installed missing dependencies inside the HPC Jupyter environment:\n",
    "\n",
    "  bash\n",
    "  pip install --user accelerate\n",
    "  pip install --user --upgrade transformers torch pandas scikit-learn\n",
    "  \n",
    "* Verified versions:\n",
    "  accelerate 1.0.1, transformers 4.46.3, torch 2.4.1+cu121.\n",
    "\n",
    "### 6 Model and tokenizer\n",
    "\n",
    "* Loaded **ConfliBERT**:\n",
    "\n",
    "  python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"snowood1/ConfliBERT-cont-uncased\")\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      \"snowood1/ConfliBERT-cont-uncased\",\n",
    "      num_labels=65,\n",
    "      problem_type=\"multi_label_classification\"\n",
    "  )\n",
    "  \n",
    "\n",
    "### 7 Train/validation split and tokenization\n",
    "\n",
    "* 90 % train / 10 % validation split (train_test_split).\n",
    "* Tokenized descriptions (max_length = 256).\n",
    "\n",
    "### 8 Training configuration\n",
    "\n",
    "* Used Hugging Face Trainer API with BCEWithLogitsLoss (multi-label default).\n",
    "* Key arguments:\n",
    "\n",
    "  python\n",
    "  TrainingArguments(\n",
    "      output_dir=\"./confli_bert_vipaa\",\n",
    "      evaluation_strategy=\"epoch\",\n",
    "      save_strategy=\"epoch\",\n",
    "      load_best_model_at_end=True,\n",
    "      metric_for_best_model=\"micro/f1\",\n",
    "      learning_rate=2e-5,\n",
    "      per_device_train_batch_size=8,\n",
    "      num_train_epochs=3,\n",
    "      weight_decay=0.01,\n",
    "      fp16=True  # mixed precision on GPU\n",
    "  )\n",
    "  \n",
    "* Custom metrics: micro/macro F1, precision, recall using sigmoid + 0.4 threshold.\n",
    "\n",
    "### 9 Threshold tuning and evaluation\n",
    "\n",
    "* Post-training, we tuned the sigmoid threshold (0.2–0.6) on validation data to maximize micro-F1.\n",
    "* Produced per-label F1 reports and saved the best model/tokenizer.\n",
    "\n",
    "### 10 Output\n",
    "\n",
    "* Saved fine-tuned model directory: ./confli_bert_vipaa_best/\n",
    "  (contains config.json, pytorch_model.bin, tokenizer.*)\n",
    "* Achieved multi-label predictions for new event texts using:\n",
    "\n",
    "  python\n",
    "  preds, probs = predict_labels([\"Guerrilleros atacaron el puesto de policía...\"])\n",
    "  \n",
    "\n",
    "## 11 **Idea kryesore**\n",
    "\n",
    "We turned raw textual incident narratives into structured multi-label conflict event classifications aligned with the **VIPAA typology** and the **ConfliBERT** language representation, creating a pipeline that:\n",
    "\n",
    "1. Preprocesses and normalizes domain-specific labels.\n",
    "2. Encodes them in a multi-hot representation.\n",
    "3. Fine-tunes a Spanish conflict-domain transformer for multi-label classification.\n",
    "4. Evaluates and saves a deployable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1720b543-56cd-4cde-8090-d9ad95452176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60299bf-1fb9-438f-936d-16c2607c3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298515f7-f120-40f4-b368-742a82dd85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user accelerate\n",
    "# !pip install --user \"transformers[torch]\"\n",
    "# !pip install --user --upgrade transformers torch pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8530d02b-6bfd-4e81-8976-d3f587a4fa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate: 1.0.1\n",
      "Transformers: 4.46.3\n",
      "Torch: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "import torch\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080d5777-76bf-4a66-b691-578555502fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32435b59-21c6-4e90-a437-21512238391b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u11/bliko/.local/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/home/u11/bliko/.local/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in main dataset:\n",
      "['Fecha del hecho', 'Ubicaciones', 'P. Responsables', 'Tipificacion', 'Tipificacion_with_codes', 'Tipificacion_clean', 'Víctimas', 'Descripción', 'Acciones']\n"
     ]
    }
   ],
   "source": [
    "#load Excel files\n",
    "#  label vocabulary (list_of_labels.xlsx) \n",
    "labels_df = pd.read_excel(\"list_of_labels.xlsx\")\n",
    "\n",
    "#  main dataset (vipaa_for_training.xlsx) \n",
    "data_df = pd.read_excel(\"vipaa_for_training.xlsx\")\n",
    "\n",
    "print(\"Columns in main dataset:\")\n",
    "print(data_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92969702-581c-4068-afee-82751293099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select label column\n",
    "label_col = \"Tipificacion_with_codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a42028e-d99d-468c-be4b-7acfa61792e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After extraction:\n",
      "0    [BIENES CIVILES, ATAQUE A OBRAS E INST. QUE CO...\n",
      "1    [LESIÓN A PERSONA PROTEGIDA, LESIÓN A PERSONA ...\n",
      "2                            [BIENES CIVILES, PILLAJE]\n",
      "Name: Tipificacion_with_codes, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#extract label names (remove the D:2:80 codes etc.)\n",
    "\n",
    "def extract_label_names(s):\n",
    "    \"\"\"\n",
    "    Takes a string like:\n",
    "    'D:2:80 BIENES CIVILES, D:2:801 ATAQUE A OBRAS E INST. QUE CONT. FUERZAS PELIGR.'\n",
    "    and returns:\n",
    "    ['BIENES CIVILES', 'ATAQUE A OBRAS E INST. QUE CONT. FUERZAS PELIGR.']\n",
    "    \"\"\"\n",
    "    parts = re.split(r\",\\s*\", str(s))\n",
    "    labels = []\n",
    "    for p in parts:\n",
    "        match = re.sub(r\"^[A-Z]:\\d+:\\d+\\s*\", \"\", p).strip()\n",
    "        if match:\n",
    "            labels.append(match.upper())\n",
    "    return labels\n",
    "\n",
    "#apply extraction\n",
    "data_df[label_col] = data_df[label_col].fillna(\"\").apply(extract_label_names)\n",
    "print(\"After extraction:\")\n",
    "print(data_df[label_col].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2bfcae-748d-4b3d-87e8-d1d90c8d95a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After flattening:\n",
      "0    [BIENES CIVILES, ATAQUE A OBRAS E INST. QUE CO...\n",
      "1    [LESIÓN A PERSONA PROTEGIDA, LESIÓN A PERSONA ...\n",
      "2                            [BIENES CIVILES, PILLAJE]\n",
      "Name: Tipificacion_with_codes, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#flatten nested lists (fix [['A','B']] -> ['A','B'])\n",
    "\n",
    "def flatten_nested_lists(x):\n",
    "    if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "data_df[label_col] = data_df[label_col].apply(flatten_nested_lists)\n",
    "\n",
    "print(\"After flattening:\")\n",
    "print(data_df[label_col].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c41bda6-c84d-4762-9feb-56fc8f2f705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 65 unique labels from list_of_labels.xlsx\n",
      "[('ASESINATO', 0), ('EJECUCION EXTRAJUDICIAL', 1), ('HOMICIDIO INTENCIONAL DE PERSONA PROTEGIDA', 2), ('COLECTIVO AMENAZADO', 3), ('AMENAZA', 4)]\n"
     ]
    }
   ],
   "source": [
    "#normalize label vocabulary and build mappings\n",
    "\n",
    "\n",
    "def normalize_label(s):\n",
    "    s = str(s).strip().upper()\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                if unicodedata.category(c) != 'Mn')\n",
    "    return s\n",
    "\n",
    "labels_df[\"actions_clean\"] = labels_df[\"actions_clean\"].apply(normalize_label)\n",
    "labels_df[\"label_id\"] = labels_df.index\n",
    "\n",
    "label_map = dict(zip(labels_df[\"actions_clean\"], labels_df[\"label_id\"]))\n",
    "id2label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "print(f\"Loaded {len(label_map)} unique labels from list_of_labels.xlsx\")\n",
    "print(list(label_map.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b3048f5-8cc8-4ee7-b2ce-0964f0bf60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample encoded labels:\n",
      "                                         Descripción Tipificacion_with_codes\n",
      "0  Guerrilleros de las FARC-EP incursionaron en l...                 [6, 36]\n",
      "1  Guerrilleros de las FARC-EP bloquearon la vía ...    [13, 13, 13, 13, 11]\n",
      "2  Guerrilleros de las FARC-EP irrumpieron en la ...                 [6, 11]\n",
      "3  Guerrilleros del Frente Manuel Vásquez Castaño...                 [6, 11]\n",
      "4  Guerrilleros del Frente Ernesto Che Guevara de...                     [2]\n"
     ]
    }
   ],
   "source": [
    "#encode each case's label list as numeric IDs\n",
    "\n",
    "def encode_labels(label_list):\n",
    "    normed = [normalize_label(l) for l in label_list]\n",
    "    return [label_map[l] for l in normed if l in label_map]\n",
    "\n",
    "data_df[label_col] = data_df[label_col].apply(encode_labels)\n",
    "\n",
    "print(\"Sample encoded labels:\")\n",
    "print(data_df[[\"Descripción\", label_col]].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffbad3d4-5d51-4e31-aee2-31a2ee38fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-hot matrix created\n",
      "y shape: (45511, 65)\n",
      "Example row: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#build multi-hot matrix for training\n",
    "\n",
    "num_labels = len(label_map)\n",
    "y = np.zeros((len(data_df), num_labels))\n",
    "\n",
    "for i, label_ids in enumerate(data_df[label_col]):\n",
    "    for lid in label_ids:\n",
    "        y[i, lid] = 1\n",
    "\n",
    "print(\"Multi-hot matrix created\")\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Example row:\", y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7218d638-6103-4f69-903c-87ff4ef45a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[label_col] = data_df[label_col].apply(lambda lst: sorted(list(set(lst))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8ad2a2-bcc7-4d12-9bdb-3141016ae2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#de-dup labels per row, then rebuild y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a538e6ae-2a51-48a0-899c-ca2a32cc3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (45511, 65)\n"
     ]
    }
   ],
   "source": [
    "#de-duplicate label IDs like [13,13,13] -> [13]\n",
    "data_df[label_col] = data_df[label_col].apply(lambda lst: sorted(set(lst)))\n",
    "\n",
    "#rebuild y (multi-hot)\n",
    "import numpy as np\n",
    "num_labels = len(label_map)\n",
    "y = np.zeros((len(data_df), num_labels), dtype=np.float32)\n",
    "for i, label_ids in enumerate(data_df[label_col]):\n",
    "    for lid in label_ids:\n",
    "        y[i, lid] = 1.0\n",
    "\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d984965b-b0a7-4dc0-8eb1-c3200139b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"snowood1/ConfliBERT-cont-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "texts = data_df[\"Descripción\"].astype(str).tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, y, test_size=0.10, random_state=42, stratify=(y.sum(axis=1) > 0)\n",
    ")\n",
    "\n",
    "train_enc = tokenizer(\n",
    "    X_train, truncation=True, padding=True, max_length=256, return_tensors=\"pt\"\n",
    ")\n",
    "val_enc = tokenizer(\n",
    "    X_val, truncation=True, padding=True, max_length=256, return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cd387ff-01ef-4b12-aa47-2ab9f5d13077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VIPAADataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = VIPAADataset(train_enc, y_train)\n",
    "val_ds   = VIPAADataset(val_enc,   y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dabca6c-710c-4a58-a937-994c326179ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snowood1/ConfliBERT-cont-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels = y.shape[1],\n",
    "    problem_type = \"multi_label_classification\",\n",
    "    id2label = id2label,\n",
    "    label2id = {v:k for k,v in id2label.items()}\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "#default BCEWithLogitsLoss is used by Trainer for multi-label\n",
    "SIGMOID_THRESH = 0.40  # I'll tune this later\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))          # sigmoid\n",
    "    preds = (probs > SIGMOID_THRESH).astype(int)\n",
    "    return {\n",
    "        \"micro/f1\":        f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"macro/f1\":        f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"micro/precision\": precision_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"micro/recall\":    recall_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f281e93f-f162-4c27-bce3-19729ab4987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u11/bliko/.local/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/xdisk/josorio1/bliko/vipaa/model/confli_bert_vipaa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro/f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, #6 locally\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5, #beje tre ne laptop\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                 # if GPU supports it\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=2, #hiqe në laptop\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdbe90c0-2c61-4882-a969-f507b225bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "\n",
    "#create a timestamped log file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "logfile = open(f\"confliBERT_train_{timestamp}.log\", \"w\")\n",
    "\n",
    "#redirect all prints and Trainer logs to this file\n",
    "sys.stdout = logfile\n",
    "sys.stderr = logfile\n",
    "\n",
    "print(\"Training started\")\n",
    "print(\"Timestamp:\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853660a4-95b3-4e84-bc88-135a7fbcdb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14830690-6b1b-4acb-a175-acf3b05cd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed successfully\")\n",
    "logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f9bb7-7eec-4d2d-b6cb-d79572f8a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# get val logits\n",
    "pred_out = trainer.predict(val_ds)\n",
    "logits = pred_out.predictions\n",
    "labels = pred_out.label_ids\n",
    "probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "best_t, best_micro = None, -1\n",
    "for t in np.linspace(0.2, 0.6, 21):\n",
    "    pred = (probs > t).astype(int)\n",
    "    micro = f1_score(labels, pred, average=\"micro\", zero_division=0)\n",
    "    if micro > best_micro:\n",
    "        best_micro, best_t = micro, t\n",
    "\n",
    "print(f\"Best threshold: {best_t:.2f} with micro-F1={best_micro:.4f}\")\n",
    "SIGMOID_THRESH = best_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2ee3f-eeb1-4b85-8a63-423455ea761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "final_preds = (probs > SIGMOID_THRESH).astype(int)\n",
    "print(\"Final (tuned) metrics:\")\n",
    "print({\n",
    "    \"micro/f1\":        f1_score(labels, final_preds, average=\"micro\", zero_division=0),\n",
    "    \"macro/f1\":        f1_score(labels, final_preds, average=\"macro\", zero_division=0),\n",
    "    \"micro/precision\": precision_score(labels, final_preds, average=\"micro\", zero_division=0),\n",
    "    \"micro/recall\":    recall_score(labels, final_preds, average=\"micro\", zero_division=0),\n",
    "})\n",
    "\n",
    "#optional: per-label F1 (top few labels)\n",
    "report = classification_report(\n",
    "    labels, final_preds,\n",
    "    target_names=[id2label[i] for i in range(num_labels)],\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "#show 10 most frequent labels by support\n",
    "supports = [(i, report[id2label[i]][\"support\"]) for i in range(num_labels)]\n",
    "top10 = [i for i,_ in sorted(supports, key=lambda x: x[1], reverse=True)[:10]]\n",
    "for i in top10:\n",
    "    r = report[id2label[i]]\n",
    "    print(f\"{id2label[i]:55s}  P={r['precision']:.2f}  R={r['recall']:.2f}  F1={r['f1-score']:.2f}  n={int(r['support'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116961e4-01ea-4930-ae41-4c29ec77d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./confli_bert_vipaa_best\"\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved to:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a04ed-e4dd-46f2-aa07-61b0be941100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_labels(texts, threshold=SIGMOID_THRESH, max_length=256):\n",
    "    enc = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = trainer.model(**{k: v.to(trainer.model.device) for k,v in enc.items()})\n",
    "        probs = torch.sigmoid(out.logits).cpu().numpy()\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    decoded = [[id2label[j] for j, v in enumerate(row) if v==1] for row in preds]\n",
    "    return decoded, probs\n",
    "\n",
    "sample_texts = [\n",
    "    data_df[\"Descripción\"].iloc[0],\n",
    "    \"Guerrilleros atacaron el puesto de policía y dinamitaron un banco.\",\n",
    "]\n",
    "pred_labels, pred_probs = predict_labels(sample_texts)\n",
    "for t, labs in zip(sample_texts, pred_labels):\n",
    "    print(\"\\nTEXT:\", t[:180], \"...\")\n",
    "    print(\"PRED:\", labs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d317f-15b8-4ec8-be99-5ef364190404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b0033-14f1-4cd0-934a-660f4faf153f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
